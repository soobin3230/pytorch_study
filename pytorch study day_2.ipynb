{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words로 Classification 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 3, 'No': 9, 'buena': 14, 'it': 7, 'at': 22, 'sea': 12, 'cafeteria': 5, 'Yo': 23, 'la': 4, 'to': 8, 'creo': 10, 'is': 16, 'a': 18, 'good': 19, 'get': 20, 'idea': 15, 'que': 11, 'not': 17, 'me': 0, 'on': 25, 'gusta': 1, 'lost': 21, 'Give': 6, 'una': 13, 'si': 24, 'comer': 2}\n",
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.1342 -0.1708  0.0709  0.1419 -0.1424  0.0092 -0.0419 -0.0775 -0.0104  0.0061\n",
      "-0.1135 -0.1588 -0.0201 -0.1071 -0.0779 -0.0400 -0.1216 -0.0747 -0.1461 -0.0343\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0550  0.1906  0.1876  0.0392  0.1406 -0.0721 -0.0235  0.1739  0.0083  0.0809\n",
      " 0.0849 -0.1783 -0.1777 -0.0166 -0.0925  0.0495  0.0466  0.0962 -0.0931  0.1070\n",
      "\n",
      "Columns 20 to 25 \n",
      " 0.1462  0.0792  0.0019 -0.0234  0.1682 -0.1821\n",
      " 0.1481  0.0124 -0.1238 -0.1128 -0.1079 -0.0308\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      "-0.1329\n",
      " 0.0340\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 26]\n",
      "\n",
      "Variable containing:\n",
      "-0.6325 -0.7577\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 문장이 spanish인지 english인지 classification 하는 문제입니다.\n",
    "\n",
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word들을 integer 값으로 mapping 해줍니다.\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # 잊지말기 !!! nn.Module 상속 후 super로 불러오기\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # 간단한 MLP 모델입니다. (layer 하나)\n",
    "        # BoW이기 때문에 voca_size가 곧 벡터의 길이가 됩니다.\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # log softmax를 거친 vector의 확률값을 뱉어냅니다.\n",
    "        return F.log_softmax(self.linear(bow_vec))\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param) # W, b의 값을 차례대로 출력합니다.\n",
    "\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "print(bow_vector[0])\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     1     1     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     1     1     0\n",
      "[torch.FloatTensor of size 1x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0827 -2.5335\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    1     0     0     0     0     0     0     1     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     1     0     0     0     0     1     0     0     0     1\n",
      "[torch.FloatTensor of size 1x26]\n",
      "\n",
      "Variable containing:\n",
      "-3.4761 -0.0314\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.5057\n",
      "-0.3658\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Variable containing:\n",
      "-0.0712 -2.6770\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-3.6942 -0.0252\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.5414\n",
      "-0.4015\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2517  0.3950  0.6368  0.7077  0.4234  0.5750 -0.7252 -1.0683 -1.0012  0.1849\n",
      " 0.0039 -0.7246 -0.5859 -0.6730 -0.6437 -0.6058  0.5617  0.9161  0.8447 -0.2131\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.5414  0.6770  0.3664  0.5255  0.6269  0.1067 -0.3311 -0.1337 -0.2993 -0.2267\n",
      "-0.4015 -0.6647 -0.3565 -0.5029 -0.5788 -0.1293  0.3542  0.4037  0.2144  0.4146\n",
      "\n",
      "Columns 20 to 25 \n",
      "-0.1613 -0.2284 -0.3057 -0.0234  0.1682 -0.1821\n",
      " 0.4557  0.3200  0.1837 -0.1128 -0.1079 -0.0308\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training 전에 test를 해보겠습니다.\n",
    "for instance, label in test_data:\n",
    "    print make_bow_vector(instance, word_to_ix)\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "# training 전의 creo에 대한 log probability를 찍어봅니다\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "# Negative loglikelihood loss를 사용합니다.\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 보통 100 epoch는 실제 데이터에 적합하지 않습니다. 약 5~30의 epoch수를 사용합니다.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Pytorch는 gradient가 중첩되기때문에 매 step마다 gradient를 초기화 해주어야 합니다.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Variable로 input vector와 label들을 wrap 해줍니다.\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "\n",
    "        # forward pass로 계산해줍니다.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # loss를 계산하고 gradient를 계산한 후에 optimizer로 gradient를 업데이트 하는 과정입니다.\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 다시 model에 test data를 넣어봅니다.\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "# creo는 원래 spanish인데, weight값이 spanish쪽이 높아진 것으로 보아 잘 학습된 것을 알 수 있습니다.\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "print next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding 후 n-gram LM 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.8690  1.6708 -0.1012  0.6869 -1.1280\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "# tf.get_varaible(\"embedding_table\", [voca_size, embedding_size]) 와 동일합니다.\n",
    "# 즉, embedding table을 만듭니다. parameters는 차례로 vocab(characters)의 unique 갯수, embedding vector dimension 입니다.\n",
    "embeds = nn.Embedding(2, 5)\n",
    "\n",
    "# lookup은 indexing 하면 됩니다.\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "hello_embed = embeds(autograd.Variable(lookup_tensor))\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# 셰익스피어 모네 data 입니다.\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "# ((2번째 전의 단어, 1번째 전의 단어), 타겟 단어)로 튜플을 만듭니다.\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "\n",
    "# 3개만 프린트 해보겠습니다.\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['all', 'brow,', 'being', 'couldst', 'treasure', 'Proving', 'to', 'field,', 'worth', 'his', 'thine!', 'lies,', 'Where', 'dig', 'succession', 'small', 'praise.', 'where', 'old', 'fair', 'see', 'Thy', \"deserv'd\", 'sum', 'shall', 'forty', 'new', 'be', 'asked,', 'days;', 'This', 'Were', 'say,', 'by', 'on', 'thou', 'of', 'thine', 'own', 'gazed', 'within', 'When', 'art', 'now,', 'trenches', \"feel'st\", 'much', 'more', 'held:', 'count,', 'it', 'warm', \"beauty's\", 'child', 'an', \"youth's\", 'And', 'made', 'How', \"'This\", 'praise', 'were', 'eyes,', 'my', 'old,', 'and', 'use,', 'mine', 'deep', 'livery', 'To', 'shame,', 'in', 'Then', 'all-eating', 'sunken', 'Shall', 'make', 'when', 'thriftless', 'answer', 'lusty', 'beauty', 'besiege', 'weed', 'Will', \"excuse,'\", 'blood', 'winters', 'a', 'thy', 'proud', 'cold.', \"totter'd\", 'so', 'the', 'If'])\n"
     ]
    }
   ],
   "source": [
    "# set이라는 좋은 함수가 있다는 걸 오늘 처음 알았네요. 자동으로 unique한 애들만 골라줍니다\n",
    "vocab = set(test_sentence)\n",
    "print vocab\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrigramClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, context_size):\n",
    "        super(TrigramClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc = nn.Linear(context_size * embedding_size, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embed = self.embedding(inputs).view((1,-1))\n",
    "        out = F.relu(self.fc(embed))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "model = TrigramClassifier(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.80429686469\n",
      "2.79989315488\n",
      "2.81764824557\n",
      "2.80762083901\n",
      "2.80451477984\n",
      "2.80253405706\n",
      "2.80112398949\n",
      "2.80018622932\n",
      "2.79969908038\n",
      "2.79953677681\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss_per_epoch = 0.0\n",
    "    for _input, label in trigrams:\n",
    "        model.zero_grad()\n",
    "        _input = autograd.Variable(torch.LongTensor([word_to_ix[w] for w in _input]))\n",
    "        \n",
    "        prob = model(_input)\n",
    "        label = autograd.Variable(torch.LongTensor([word_to_ix[label]]))\n",
    "        \n",
    "        loss = loss_function(prob, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        loss_per_epoch += loss.data[0]\n",
    "        \n",
    "    print loss_per_epoch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [autograd.Variable(torch.randn((1,3))) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       "  0.4438  0.3946  1.4681\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       " -0.1383 -0.7589 -0.7654\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       " -0.6630 -0.6874 -0.5107\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       "  1.8368 -0.2794 -0.8937\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       " -0.6421  0.4721 -2.9939\n",
       " [torch.FloatTensor of size 1x3]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Everybody': 5, 'ate': 2, 'apple': 4, 'that': 7, 'read': 6, 'dog': 1, 'book': 8, 'the': 3, 'The': 0}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# weight가 어떻게 변하는지 보기위해 dimension을 작게 가져감\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_units, target_size):\n",
    "        # 무조건 합니다\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        # lstm num_units (hidden dimesion) 설정\n",
    "        self.num_units = num_units\n",
    "        # embedding table을 만들어줍니다.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        # lstm을 선언합니다. [input dimension, hidden dimension]\n",
    "        self.lstm = nn.LSTM(embedding_size, num_units)\n",
    "        # classification용 output linear layer입니다.\n",
    "        self.output = nn.Linear(num_units, target_size)\n",
    "        \n",
    "        # pytorch는 lstm cell의 hidden을 따로 선언해주고 처음에 initialize 해야 합니다.\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # (h_n, c_n) pair 입니다.\n",
    "        return (autograd.Variable(torch.zeros(1,1,self.num_units)), autograd.Variable(torch.zeros(1,1,self.num_units)))\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # embedding table에서 input vector를 꺼내옵니다. [seq_len, batch_size, embedding_size]\n",
    "        embed = self.embedding(inputs)\n",
    "        \n",
    "        # output: [seq_len, batch_size, num_units], self.hidden: [num_layers, batch_size, num_units]\n",
    "        output, self.hidden = self.lstm(embed.view(len(inputs), 1, -1), self.hidden)\n",
    "        \n",
    "        # softmax layer\n",
    "        output = self.output(output.view(len(inputs),-1))\n",
    "        output = F.log_softmax(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM, len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(300):\n",
    "    for inputs, targets in training_data:\n",
    "        # index의 list로 변환\n",
    "        inputs = prepare_sequence(inputs, word_to_ix)\n",
    "        targets = prepare_sequence(targets, tag_to_ix)\n",
    "        \n",
    "        # 무조건 해줍니다.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # lstm hidden state를 초기화해줍니다. (이거 질문)\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        output = model(inputs)\n",
    "\n",
    "        loss = loss_function(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
