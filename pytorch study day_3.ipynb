{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence model with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터를 [여기서](https://download.pytorch.org/tutorial/data.zip) 먼저 다운받고 현재 디렉토리에 압축을 풉니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 필요한 모듈들을 import 해줍니다.\n",
    "\n",
    "import unicodedata\n",
    "import re, random\n",
    "from io import open\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2:\"EOS\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data를 읽어오는 함수입니다. 본 예제에서는 english-to-french 데이터를 사용합니다.\n",
    "# 사실 pre-processing 함수는 본인이 데이터에 맞게 직접 만들어야 하기 때문에 자세히 다루지 않겠습니다.\n",
    "\n",
    "# data가 unicode로 되어있으므로 ascii 형식으로 바꿔주는 함수입니다.\n",
    "# http://stackoverflow.com/a/518232/2809427 참조\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# letter가 아닌 character를 지우고, 대문자를 소문자로 바꾸는 함수입니다.\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # line별 데이터를 읽어옵니다.\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # pair로 나누고, normalize 해줍니다.\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # 위에서 선언한 Lang class의 instance를 만들어줍니다. 이제 각 언어별 dictionary가 생성되었습니다.\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch tutorial에서는 max length=10으로 정해주고 데이터를 넣어주었지만,\n",
    "# 본 예제에서는 mini-batch별로 max length를 처리해주는 후처리 과정을 거칩니다.\n",
    "# prefix만 filtering 해줍니다.\n",
    "\n",
    "def filterPair(p):\n",
    "    eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    "    )\n",
    "    return p[0].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터를 불러옵니다. (lang instance 불러오기)\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mini-batch별로 max_length 만큼의 ndarray data를 만들어 list에 추가해주는 작업입니다.\n",
    "# [batch_size, max_length_per_minibatch] * num_batch\n",
    "\n",
    "def get_data(batch_size=32):\n",
    "\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "    num_batch = len(pairs) // batch_size\n",
    "    en_data = []\n",
    "    fr_data = []\n",
    "    for i in xrange(num_batch):\n",
    "        en = [pair[0] for pair in pairs[i * batch_size:(i + 1) * batch_size]]\n",
    "        fr = [pair[1] for pair in pairs[i * batch_size:(i + 1) * batch_size]]\n",
    "        en_batch = np.zeros([batch_size, max(map(len, en))], dtype=np.int64)\n",
    "        fr_batch = np.zeros([batch_size, max(map(len, en))], dtype=np.int64)\n",
    "        for j in xrange(batch_size):\n",
    "            en_indexes = indexesFromSentence(input_lang, en[j])\n",
    "            fr_indexes = indexesFromSentence(output_lang, fr[j])\n",
    "            en_batch[j,:len(en_indexes)] = en_indexes\n",
    "            fr_batch[j,:len(fr_indexes)] = fr_indexes\n",
    "\n",
    "        en_data.append(en_batch)\n",
    "        fr_data.append(fr_batch)\n",
    "\n",
    "    return en_data, fr_data, input_lang, output_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder network를 만드는 과정입니다.\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, batch_size=16):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # nn.GRU는 [embedding_size, hidden_size, n_layers]를 보통 파라미터로 받습니다.\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        # gru cell에 넣어줄 embedding은 time-major입니다. [time_step=1(1로 하는게 무난합니다.), batch_size, embedding_size]\n",
    "\n",
    "        embedded = self.embedding(input_).view(1, self.batch_size, -1)\n",
    "        output = embedded\n",
    "\n",
    "        output, hidden_state = self.gru(output, hidden)\n",
    "\n",
    "        return output, hidden_state\n",
    "\n",
    "    def initHidden(self):\n",
    "        # [n_layers, batch_size, hidden_size]짜리 init_hidden_state를 생성하는 과정입니다.\n",
    "        result = Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.0514 -0.0361  0.0485  0.1221 -0.0696 -0.0777  0.0453  0.1357 -0.0286\n",
      "  0.0661 -0.0581 -0.0224  0.1055  0.0035 -0.2160  0.0704  0.0738 -0.2125\n",
      "  0.0245 -0.0121  0.0045  0.0855 -0.0105 -0.1455  0.1022  0.0227 -0.0684\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1761 -0.0087 -0.1104  0.0572 -0.0073 -0.0011 -0.1049  0.0423  0.0124\n",
      " -0.1206  0.0689 -0.0783  0.1225 -0.1418  0.0509 -0.0853 -0.1425  0.0239\n",
      " -0.1149  0.0512 -0.0858  0.0700 -0.0745  0.0282  0.0598  0.0066  0.0053\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0015 -0.1996\n",
      " -0.0694 -0.0994\n",
      "  0.1514 -0.1362\n",
      "[torch.FloatTensor of size 1x3x20]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.3545  0.2560  0.0706  0.1453 -0.1174  0.1864  0.3222  0.0279  0.2001\n",
      "  0.4105  0.1764  0.1662 -0.3318  0.1165 -0.6389  0.3788  0.0878  0.0966\n",
      "  0.3668 -0.2130  0.2221 -0.2019 -0.2628 -0.1211 -0.1323 -0.0491  0.2152\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.2953 -0.0827  0.2504  0.0836  0.2076  0.0175  0.0700 -0.3431 -0.2389\n",
      " -0.2807  0.2767 -0.3681  0.2505 -0.0952 -0.1282 -0.6039 -0.0218  0.3391\n",
      "  0.3799  0.0856  0.3383 -0.0037 -0.0350 -0.1777 -0.1709 -0.0738  0.0200\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.1244 -0.1955\n",
      "  0.1507  0.1975\n",
      " -0.3445 -0.3310\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.0514 -0.0361  0.0485  0.1221 -0.0696 -0.0777  0.0453  0.1357 -0.0286\n",
      "  0.0661 -0.0581 -0.0224  0.1055  0.0035 -0.2160  0.0704  0.0738 -0.2125\n",
      "  0.0245 -0.0121  0.0045  0.0855 -0.0105 -0.1455  0.1022  0.0227 -0.0684\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1761 -0.0087 -0.1104  0.0572 -0.0073 -0.0011 -0.1049  0.0423  0.0124\n",
      " -0.1206  0.0689 -0.0783  0.1225 -0.1418  0.0509 -0.0853 -0.1425  0.0239\n",
      " -0.1149  0.0512 -0.0858  0.0700 -0.0745  0.0282  0.0598  0.0066  0.0053\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0015 -0.1996\n",
      " -0.0694 -0.0994\n",
      "  0.1514 -0.1362\n",
      "[torch.FloatTensor of size 2x3x20]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예시입니다. [3,1]짜리 dimension을 가진 mini-batch를 받아서 [1, 3, 10]의 embedding을 만들고, [1,3,20] 짜리 output을 뱉어냅니다.\n",
    "# state는 [n_layer, batch_size, hidden_size]의 사이즈를 가집니다.\n",
    "e = EncoderRNN(10,20,2, batch_size=3)\n",
    "\n",
    "o, h = e(Variable(torch.LongTensor([[1],[2],[3]])),e.initHidden())\n",
    "\n",
    "print(o)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "  <img src=\"https://raw.githubusercontent.com/soobin3230/pytorch_study/master/png/attention.png\"/>\n",
    "</p>\n",
    "<p>\n",
    "  <img src=\"https://raw.githubusercontent.com/soobin3230/pytorch_study/master/png/algorithm.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # hidden_size * 2를 해주는 이유는 attention vector를 input과 concat시켜 그다음 input에 넣어줄 것이기 때문입니다.\n",
    "        # 그림 참조\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size)\n",
    "\n",
    "        # Attention Mechanism\n",
    "        # Attention matrix를 만드는데 필요한 layer들을 선언해줍니다.\n",
    "\n",
    "        # linear layer를 거쳐 memory를 만들어줍니다. hs_bar입니다.\n",
    "        self.get_keys = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # score를 계산할 때의 W1, W2 matrix입니다.\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        # score를 계산할 때 사용되는 learnable parameter입니다.\n",
    "        self.v = Variable(torch.randn(hidden_size, 1), requires_grad=True)\n",
    "\n",
    "        if use_cuda:\n",
    "            self.v = self.v.cuda()\n",
    "\n",
    "        # 최종적으로 Attention vector를 만들 때 사용하는 matrix입니다.\n",
    "        self.get_attn_vector = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        # 마지막 fc layer로 prediction을 뱉습니다.\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_, hidden, enc_outs):\n",
    "\n",
    "        seq_len, batch_size = enc_outs.data.size()[:2]\n",
    "\n",
    "        # 전 timestep의 cell에서 뱉은 h_t를 받아옵니다.\n",
    "        h_t = hidden  # hidden state of RNN\n",
    "\n",
    "        attn_weights = []\n",
    "        # Attention Mechanism\n",
    "        # encoder의 각 timestep 0~s까지의 alpha_ts 값을 구하는 과정입니다.\n",
    "        for j in range(seq_len):\n",
    "            # memory를 만들어줍니다.\n",
    "            memory = self.get_keys(enc_outs[j]) # [batch_size, hidden_size]\n",
    "\n",
    "            # memory와 query를 concat하여 score를 계산해줍니다.\n",
    "            score = F.tanh(self.attn(torch.cat([memory, h_t[0]], -1)))  # [batch_size, hidden_size]\n",
    "\n",
    "            # v를 matrix_mul 해줍니다.\n",
    "            score = torch.mm(score, self.v)  # [batch_size, hidden_size] * [hidden_size, 1(timestep)] = [batch_size, 1(timestep)]\n",
    "\n",
    "            # softmax 값을 구하기 위해서는 encoder timestep 전부의 alpha값이 필요하므로 list에 저장해두고 for loop를 돌립니다.\n",
    "            attn_weights.append(score)\n",
    "\n",
    "        # 만들어준 list를 하나로 합칩니다.\n",
    "        attn_weights = torch.cat(attn_weights, 1)  # [batch_size, max_length_encoder_minibatch]\n",
    "\n",
    "        # alpha_ts가 vector의 형태로 나옵니다. \n",
    "        attn_weights = F.softmax(attn_weights)  # [batch_size, max_length_encoder_minibatch]\n",
    "\n",
    "        # context vector를 만듭니다. batch별 matrix_mul입니다. \n",
    "        context_vector = torch.bmm(attn_weights.view(batch_size, 1, -1), enc_outs.view(batch_size,seq_len,-1))  # [batch_size,1,max_length_encoder_minibatch] * [batch_size,max_length_encoder_minibatch,hidden_size]\n",
    "\n",
    "        # attention vector를 만듭니다.\n",
    "        attention_vector = F.tanh(self.get_attn_vector(torch.cat([context_vector, h_t.view(batch_size,1,-1)], -1).view(batch_size, -1)))\n",
    "\n",
    "        # decoder input embedding을 만듭니다.\n",
    "        embedded = self.embedding(input_)  # [batch_size, 1, hidden_size] \n",
    "\n",
    "        # gru cell에 넣어줄 input을 attention vector와 concat 해줍니다.\n",
    "        comb = torch.cat((attention_vector.view(batch_size, 1, -1), embedded), -1)  # [batch_size, 1, hidden_size * 2] \n",
    "\n",
    "        # gru cell에 input, hidden을 넣어줍니다. time_major로 바꿔줍니다. (grucell batch_first=True error)\n",
    "        output, hidden = self.gru(comb.view(1, batch_size, -1), h_t.view(1, batch_size, -1))  # [1,batch_size, hidden_size*2], [1, batch_size, hidden_size]\n",
    "\n",
    "        # 각 timestep별 최종 logit을 계산합니다.\n",
    "        output = self.fc(output.view(batch_size, -1)).view(batch_size, 1, -1)  # [batch_size, 1, voca_size]\n",
    "\n",
    "        # NLL loss logit\n",
    "        output = F.log_softmax(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "   -1.0728 -1.0688 -1.1687 -1.2829 -1.2584 -0.9902 -0.9625 -0.9818 -0.9779\n",
       " \n",
       " Columns 9 to 9 \n",
       "   -0.9761\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "   -0.9331 -1.1643 -1.0486 -0.8193 -1.0518 -1.2424 -1.2937 -1.2349 -1.0908\n",
       " \n",
       " Columns 9 to 9 \n",
       "   -1.0597\n",
       " \n",
       " (2 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "   -1.3294 -1.0659 -1.0824 -1.2658 -1.0035 -1.0793 -1.0677 -1.0952 -1.2450\n",
       " \n",
       " Columns 9 to 9 \n",
       "   -1.2849\n",
       " [torch.FloatTensor of size 3x1x10], Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.2160  0.0083 -0.2469 -0.1119 -0.0117 -0.3233  0.0327 -0.3268  0.1438\n",
       "  -0.0880  0.3351 -0.0123 -0.1893 -0.4085 -0.2786 -0.0941  0.1479  0.2131\n",
       "   0.0193  0.2138  0.1761 -0.2906 -0.3902 -0.5038 -0.0908 -0.0331  0.0792\n",
       " \n",
       " Columns 9 to 17 \n",
       "   -0.0759 -0.1373  0.1704  0.2166  0.0770 -0.0065 -0.1758  0.0538 -0.3487\n",
       "  -0.4121  0.1189  0.3891 -0.3790  0.0467 -0.3752  0.1173  0.2407 -0.2611\n",
       "   0.2380 -0.1599  0.2320  0.0388  0.1101  0.1141  0.1856 -0.3472 -0.3219\n",
       " \n",
       " Columns 18 to 26 \n",
       "   -0.0973  0.1050  0.0875 -0.5104 -0.0466 -0.1838 -0.2843  0.4096 -0.0756\n",
       "  -0.1716  0.1950 -0.4201  0.3052  0.3788 -0.0884  0.0275  0.1654 -0.4075\n",
       "  -0.4225 -0.0266 -0.4159 -0.1122 -0.3315 -0.1688  0.1764  0.3318 -0.0199\n",
       " \n",
       " Columns 27 to 29 \n",
       "   -0.3472  0.3915  0.1694\n",
       "   0.2712  0.4272  0.1244\n",
       "   0.1548  0.3546 -0.5972\n",
       " [torch.FloatTensor of size 1x3x30], Variable containing:\n",
       "  0.0834  0.2528  0.2120  0.2545  0.1973\n",
       "  0.1487  0.2663  0.2410  0.1120  0.2321\n",
       "  0.2051  0.1817  0.2215  0.2084  0.1833\n",
       " [torch.FloatTensor of size 3x5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시입니다. encoder length = 5, batch_size = 3\n",
    "de = AttnDecoderRNN(30,10)\n",
    "de(Variable(torch.LongTensor([[1],[2],[3]])), Variable(torch.zeros([1,3,30])), Variable(torch.rand([5,3,30])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentenced의 word들을 index로 만들어주는 함수입니다.\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "# numpy data를 torch Variable로 만들어주는 함수입니다.\n",
    "def np_to_variable(var):\n",
    "    return Variable(torch.from_numpy(np.expand_dims(var, axis=1))).cuda() if use_cuda else Variable(torch.from_numpy(np.expand_dims(var, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 135842 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "('eng', 3553)\n",
      "('fra', 5393)\n",
      "2.78182601929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3a43c63363fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# 1 mini_batch의 loss로 gradient를 계산합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# weight를 update 해줍니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hyperparameter들을 선언해줍니다. 따로 parser를 두어도 좋습니다.\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "hidden_size = 128\n",
    "\n",
    "# mini_batch 별로 split한 data를 불러옵니다.\n",
    "en_data, fr_data, input_lang, output_lang = get_data(batch_size=batch_size)\n",
    "\n",
    "# encoder instance를 만들어줍니다. GPU를 쓸때는 network도 .cuda()를 해주어야 합니다.\n",
    "encoder = EncoderRNN(input_lang.n_words,hidden_size,batch_size=batch_size)\n",
    "encoder = encoder.cuda() if use_cuda else encoder\n",
    "\n",
    "# decoder instance를 만들어줍니다. 마찬가지로 GPU를 쓸때는 .cuda()를 해주어야 합니다.\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words)\n",
    "decoder = decoder.cuda() if use_cuda else decoder\n",
    "\n",
    "# optimizer들을 선언해줍니다.\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss criterion을 선언해줍니다.\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# training을 시작합니다.\n",
    "for _ in range(num_epochs):\n",
    "    \n",
    "    # 1 epoch 만큼입니다. 여기서 data shuffling을 해주어야 합니다. code 업데이트 추후 예정\n",
    "    for j in range(len(en_data)):\n",
    "\n",
    "        # mini_batch 마다 encoder_hidden, gradients, loss를 초기화 해줍니다.\n",
    "        h = encoder.initHidden()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(en_data[j].shape[1]):\n",
    "            # batch별 max_length만큼 encoder에 넣어줍니다.\n",
    "            encoder_input = np_to_variable(en_data[j][:,i])\n",
    "            encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "            o, h = encoder(encoder_input,h)\n",
    "            outputs.append(o)\n",
    "        \n",
    "        # mini_batch max_length만큼의 outputs를 concat하여 하나의 tensor로 만듭니다.\n",
    "        outputs = torch.cat(outputs)\n",
    "        outputs = outputs.cuda() if use_cuda else outputs\n",
    "\n",
    "        # teacher forcing은 하지 않았습니다. SOS token만 넣어주고 학습시켜줍니다.\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token] for _ in range(batch_size)]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        \n",
    "        for k in range(fr_data[j].shape[1]):\n",
    "            ou, h, at = decoder(decoder_input, h, outputs)\n",
    "            topv, topi =  ou.data.topk(1)\n",
    "            next_input = topi[:,0]\n",
    "            \n",
    "            # next_input에 이전 step의 output을 넣어줍니다.\n",
    "            decoder_input = Variable(next_input)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "            # loss를 계산해줍니다.\n",
    "            loss += criterion(ou.view(batch_size,-1), np_to_variable(fr_data[j][:, k]).contiguous().view(batch_size))\n",
    "        \n",
    "        # 1 mini_batch의 loss로 gradient를 계산합니다.\n",
    "        loss.backward()\n",
    "\n",
    "        # weight를 update 해줍니다.\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        # logging\n",
    "        if j % 100 ==0: print loss.data[0] / fr_data[j].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation code는 추후 update 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
